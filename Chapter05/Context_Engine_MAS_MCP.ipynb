{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNU0I6xrrBj/Wca8WoMSZtN"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Context Engine : MAS & MCP\n",
        "\n",
        "Copyright 2025, Denis Rothman\n",
        "\n",
        "\n",
        "**Building the Context Engine**\n",
        "\n",
        "*From Prototype to pre-production: Hardening the Context Engine*\n",
        "\n",
        "\n",
        "This notebook implements the evolution of a \"Context Engine\" from a prototype to a robust, pre-production multi-agent AI system. It showcases a sophisticated architecture featuring a central Planner that dynamically creates execution strategies based on high-level goals. The system deploys specialized agentsâ€”a Librarian for stylistic blueprints, a Researcher for factual retrieval, and a Writer for content generation.\n",
        "\n",
        "A core principle demonstrated here is **dependency injection**, where all configurations, clients, and model parameters are explicitly passed, eliminating reliance on global variables and enhancing modularity. The Planner is hardened to produce structured JSON output, ensuring reliable parsing and execution. The agents collaborate through **context chaining**, allowing the output of one step to seamlessly become the input for the next.\n",
        "\n",
        "This engine leverages a Retrieval-Augmented Generation (RAG) pipeline, using Pinecone for vector search across distinct knowledge and context namespaces. Communication is standardized via a custom Model Context Protocol (MCP). The final execution cell provides a practical example, tasking the engine with generating a creative story about the Apollo 11 moon landing, showcasing the synergy of the entire hardened system.\n"
      ],
      "metadata": {
        "id": "hfzZQTDjpfu6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GitHub"
      ],
      "metadata": {
        "id": "a455hFQaWMUw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Downloading the utilities and the agents ---\n",
        "\n",
        "# The !curl command is a simple and effective way to download raw files from a public GitHub repo.\n",
        "# The -L flag ensures that it follows any redirects.\n",
        "\n",
        "# print(\"Downloading helper files from public repository...\")\n",
        "# !curl -L https://raw.githubusercontent.com/Denis2054/Context-Engineering/main/commons/utils.py --output utils.py\n",
        "# !curl -L https://raw.githubusercontent.com/Denis2054/Context-Engineering/main/commons/agents.py --output agents.py\n",
        "\n",
        "# print(\"âœ… Files downloaded successfully!\")"
      ],
      "metadata": {
        "id": "KkF91Ki_Xw9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "def download_private_github_file(filename, destination_path=\".\"):\n",
        "    \"\"\"\n",
        "    Downloads a file from a private GitHub repository using a token\n",
        "    stored in Colab Secrets.\n",
        "\n",
        "    Requires a secret named 'GITHUB_TOKEN' to be set in the Colab UI.\n",
        "    \"\"\"\n",
        "    # --- Configuration: Replace with your repository details ---\n",
        "    owner = \"Denis2054\"\n",
        "    repo = \"Context-Engineering\"\n",
        "    branch = \"main\"\n",
        "    # ---------------------------------------------------------\n",
        "\n",
        "    try:\n",
        "        # Securely fetch the GitHub token from Colab Secrets\n",
        "        token = userdata.get('GITHUB_TOKEN')\n",
        "        if not token:\n",
        "            raise userdata.SecretNotFoundError(\"Secret 'GITHUB_TOKEN' not found.\")\n",
        "    except userdata.SecretNotFoundError:\n",
        "        print(\"ðŸ›‘ Error: Secret 'GITHUB_TOKEN' not found.\")\n",
        "        print(\"Please add your GitHub Personal Access Token to the Colab Secrets Manager.\")\n",
        "        return\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while accessing secrets: {e}\")\n",
        "        return\n",
        "\n",
        "    # Construct the GitHub API URL for the file\n",
        "    url = f\"https://api.github.com/repos/{owner}/{repo}/contents/{filename}?ref={branch}\"\n",
        "\n",
        "    # Prepare headers for authentication and to request the raw file content\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {token}\",\n",
        "        \"Accept\": \"application/vnd.github.v3.raw\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        # Make the request to the GitHub API\n",
        "        response = requests.get(url, headers=headers)\n",
        "        response.raise_for_status()  # This will raise an HTTPError for bad responses (4xx or 5xx)\n",
        "\n",
        "        # Determine the local filename\n",
        "        local_filename = os.path.join(destination_path, os.path.basename(filename))\n",
        "\n",
        "        # Save the file content locally\n",
        "        with open(local_filename, \"wb\") as f:\n",
        "            f.write(response.content)\n",
        "\n",
        "        print(f\"âœ… Successfully downloaded '{filename}' to '{local_filename}'\")\n",
        "\n",
        "    except requests.exceptions.HTTPError as e:\n",
        "        print(f\"ðŸ›‘ Error downloading '{filename}': {e}\")\n",
        "        if e.response.status_code == 404:\n",
        "            print(\"   Please check that the owner, repo, file path, and branch are correct.\")\n",
        "        elif e.response.status_code == 401:\n",
        "            print(\"   Authentication failed. Please check if your GITHUB_TOKEN is valid and has access to the repo.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "\n",
        "\n",
        "# --- Example Usage: Download your utility files ---\n",
        "# Ensure you have set 'GITHUB_TOKEN' in your Colab Secrets before running this.\n",
        "\n",
        "download_private_github_file(\"commons/utils.py\")\n",
        "download_private_github_file(\"commons/agents.py\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9-qdFBYWNuV",
        "outputId": "1edbda68-8eaf-484f-f206-e2677f08aaa5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Successfully downloaded 'commons/utils.py' to './utils.py'\n",
            "âœ… Successfully downloaded 'commons/agents.py' to './agents.py'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.Installation and Setup 2.Initialize Clients"
      ],
      "metadata": {
        "id": "zo-XYpI5f3JX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Installation and Client Setup\n",
        "\n",
        "# Import the setup functions from your new utility file\n",
        "import utils\n",
        "\n",
        "# Run the installation\n",
        "utils.install_dependencies()\n",
        "\n",
        "# Initialize the OpenAI and Pinecone clients\n",
        "client, pc = utils.initialize_clients()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkx_Oxy7gerq",
        "outputId": "9c8f4d57-5101-4ca3-ad90-1f3ded59d5fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸš€ Installing required packages...\n",
            "âœ… All packages installed successfully.\n",
            "\n",
            "ðŸ”‘ Initializing API clients...\n",
            "   - OpenAI client initialized.\n",
            "   - Pinecone client initialized.\n",
            "âœ… Clients initialized successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.Helper Functions (LLM, Embeddings, and MCP)"
      ],
      "metadata": {
        "id": "YpnNWNgcB_jY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Imports for this section ===\n",
        "import json\n",
        "import time\n",
        "import logging\n",
        "import tiktoken\n",
        "import re\n",
        "import copy\n",
        "from tenacity import retry, stop_after_attempt, wait_random_exponential\n",
        "from openai import APIError # Import specific error for better handling\n",
        "\n",
        "# === Configure Production-Level Logging ===\n",
        "logging.basicConfig(level=logging.INFO,\n",
        "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# === LLM Interaction (Hardened with Dependency Injection) ===\n",
        "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\n",
        "def call_llm_robust(system_prompt, user_prompt, client, generation_model, json_mode=False):\n",
        "    \"\"\"\n",
        "    A centralized function to handle all LLM interactions with retries.\n",
        "    UPGRADE: Now requires the 'client' and 'generation_model' objects to be passed in.\n",
        "    \"\"\"\n",
        "    logging.info(\"Attempting to call LLM...\")\n",
        "    try:\n",
        "        response_format = {\"type\": \"json_object\"} if json_mode else {\"type\": \"text\"}\n",
        "        # UPGRADE: Uses the passed-in client and model name for the API call.\n",
        "        response = client.chat.completions.create(\n",
        "            model=generation_model,\n",
        "            response_format=response_format,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": user_prompt}\n",
        "            ],\n",
        "        )\n",
        "        logging.info(\"LLM call successful.\")\n",
        "        return response.choices[0].message.content.strip()\n",
        "    except APIError as e:\n",
        "        logging.error(f\"OpenAI API Error in call_llm_robust: {e}\")\n",
        "        raise e\n",
        "    except Exception as e:\n",
        "        logging.error(f\"An unexpected error occurred in call_llm_robust: {e}\")\n",
        "        raise e\n",
        "\n",
        "# === Embeddings (Hardened with Dependency Injection) ===\n",
        "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\n",
        "def get_embedding(text, client, embedding_model):\n",
        "    \"\"\"\n",
        "    Generates embeddings for a single text query with retries.\n",
        "    UPGRADE: Now requires the 'client' and 'embedding_model' objects.\n",
        "    \"\"\"\n",
        "    text = text.replace(\"\\n\", \" \")\n",
        "    try:\n",
        "        # UPGRADE: Uses the passed-in client and model name.\n",
        "        response = client.embeddings.create(input=[text], model=embedding_model)\n",
        "        return response.data[0].embedding\n",
        "    except APIError as e:\n",
        "        logging.error(f\"OpenAI API Error in get_embedding: {e}\")\n",
        "        raise e\n",
        "    except Exception as e:\n",
        "        logging.error(f\"An unexpected error occurred in get_embedding: {e}\")\n",
        "        raise e\n",
        "\n",
        "# === Model Context Protocol (MCP) ===\n",
        "def create_mcp_message(sender, content, metadata=None):\n",
        "    \"\"\"Creates a standardized MCP message.\"\"\"\n",
        "    return {\n",
        "        \"protocol_version\": \"2.0 (Context Engine)\",\n",
        "        \"sender\": sender,\n",
        "        \"content\": content,\n",
        "        \"metadata\": metadata or {}\n",
        "    }\n",
        "\n",
        "# === Pinecone Interaction (Hardened with Dependency Injection) ===\n",
        "def query_pinecone(query_text, namespace, top_k, index, client, embedding_model):\n",
        "    \"\"\"\n",
        "    Embeds the query text and searches the specified Pinecone namespace.\n",
        "    UPGRADE: Now requires 'index', 'client', and 'embedding_model' objects.\n",
        "    \"\"\"\n",
        "    logging.info(f\"Querying Pinecone namespace '{namespace}'...\")\n",
        "    try:\n",
        "        # UPGRADE: Passes the necessary dependencies down to get_embedding.\n",
        "        query_embedding = get_embedding(query_text, client=client, embedding_model=embedding_model)\n",
        "        # UPGRADE: Uses the passed-in index object for the query.\n",
        "        response = index.query(\n",
        "            vector=query_embedding,\n",
        "            namespace=namespace,\n",
        "            top_k=top_k,\n",
        "            include_metadata=True\n",
        "        )\n",
        "        logging.info(\"Pinecone query successful.\")\n",
        "        return response['matches']\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error querying Pinecone (Namespace: {namespace}): {e}\")\n",
        "        raise e\n",
        "\n",
        "# === Context Management Utility (New) ===\n",
        "def count_tokens(text, model=\"gpt-4\"):\n",
        "    \"\"\"Counts the number of tokens in a text string for a given model.\"\"\"\n",
        "    try:\n",
        "        encoding = tiktoken.encoding_for_model(model)\n",
        "    except KeyError:\n",
        "        # Fallback for models that might not be in the tiktoken registry\n",
        "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "    return len(encoding.encode(text))\n",
        "\n",
        "\n",
        "logging.info(\"âœ… Helper functions defined and upgraded.\")"
      ],
      "metadata": {
        "id": "FwEeJUGhCp0-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.The Specialist Agents (Upgraded for Production)"
      ],
      "metadata": {
        "id": "5SEiQlwRLCFm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#4.The Specialist Agents (Fully Upgraded with Full Dependency Injection)\n",
        "# -------------------------------------------------------------------------\n",
        "# We now complete the upgrade of our specialist agents.\n",
        "# The final step is to pass the configuration variables (like model names\n",
        "# AND namespaces) as arguments, making the agents fully self-contained and\n",
        "# removing all reliance on global variables.\n",
        "# -------------------------------------------------------------------------\n",
        "\n",
        "# === 4.1. Context Librarian Agent (Upgraded) ===\n",
        "# *** Added 'namespace_context' argument ***\n",
        "def agent_context_librarian(mcp_message, client, index, embedding_model, namespace_context):\n",
        "    \"\"\"\n",
        "    Retrieves the appropriate Semantic Blueprint from the Context Library.\n",
        "    UPGRADE: Now also accepts embedding_model and namespace configuration.\n",
        "    \"\"\"\n",
        "    logging.info(\"[Librarian] Activated. Analyzing intent...\")\n",
        "    try:\n",
        "        requested_intent = mcp_message['content'].get('intent_query')\n",
        "\n",
        "        if not requested_intent:\n",
        "            raise ValueError(\"Librarian requires 'intent_query' in the input content.\")\n",
        "\n",
        "        # UPGRADE: Pass all necessary dependencies to the hardened helper function.\n",
        "        results = query_pinecone(\n",
        "            query_text=requested_intent,\n",
        "            # *** Use the passed argument instead of the global variable ***\n",
        "            namespace=namespace_context,\n",
        "            top_k=1,\n",
        "            index=index,\n",
        "            client=client,\n",
        "            embedding_model=embedding_model\n",
        "        )\n",
        "\n",
        "        if results:\n",
        "            match = results[0]\n",
        "            logging.info(f\"[Librarian] Found blueprint '{match['id']}' (Score: {match['score']:.2f})\")\n",
        "            blueprint_json = match['metadata']['blueprint_json']\n",
        "            content = blueprint_json\n",
        "        else:\n",
        "            logging.warning(\"[Librarian] No specific blueprint found. Returning default.\")\n",
        "            content = json.dumps({\"instruction\": \"Generate the content neutrally.\"})\n",
        "\n",
        "        return create_mcp_message(\"Librarian\", content)\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"[Librarian] An error occurred: {e}\")\n",
        "        raise e\n",
        "\n",
        "# === 4.2. Researcher Agent (Upgraded) ===\n",
        "# *** 'namespace_knowledge' argument ***\n",
        "def agent_researcher(mcp_message, client, index, generation_model, embedding_model, namespace_knowledge):\n",
        "    \"\"\"\n",
        "    Retrieves and synthesizes factual information from the Knowledge Base.\n",
        "    UPGRADE: Now accepts all necessary model and namespace configurations.\n",
        "    \"\"\"\n",
        "    logging.info(\"[Researcher] Activated. Investigating topic...\")\n",
        "    try:\n",
        "        topic = mcp_message['content'].get('topic_query')\n",
        "\n",
        "        if not topic:\n",
        "            raise ValueError(\"Researcher requires 'topic_query' in the input content.\")\n",
        "\n",
        "        # UPGRADE: Pass all dependencies to the Pinecone helper.\n",
        "        results = query_pinecone(\n",
        "            query_text=topic,\n",
        "            # *** Use the passed argument instead of the global variable ***\n",
        "            namespace=namespace_knowledge,\n",
        "            top_k=3,\n",
        "            index=index,\n",
        "            client=client,\n",
        "            embedding_model=embedding_model\n",
        "        )\n",
        "\n",
        "        if not results:\n",
        "            logging.warning(\"[Researcher] No relevant information found.\")\n",
        "            return create_mcp_message(\"Researcher\", \"No data found on the topic.\")\n",
        "\n",
        "        logging.info(f\"[Researcher] Found {len(results)} relevant chunks. Synthesizing...\")\n",
        "        source_texts = [match['metadata']['text'] for match in results]\n",
        "\n",
        "        system_prompt = \"\"\"You are an expert research synthesis AI.\n",
        "        Synthesize the provided source texts into a concise, bullet-pointed summary relevant to the user's topic. Focus strictly on the facts provided in the sources. Do not add outside information.\"\"\"\n",
        "\n",
        "        user_prompt = f\"Topic: {topic}\\n\\nSources:\\n\" + \"\\n\\n---\\n\\n\".join(source_texts)\n",
        "\n",
        "        # UPGRADE: Pass all dependencies to the LLM helper.\n",
        "        findings = call_llm_robust(\n",
        "            system_prompt,\n",
        "            user_prompt,\n",
        "            client=client,\n",
        "            generation_model=generation_model\n",
        "        )\n",
        "\n",
        "        return create_mcp_message(\"Researcher\", findings)\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"[Researcher] An error occurred: {e}\")\n",
        "        raise e\n",
        "\n",
        "# === 4.3. Writer Agent (Upgraded) ===\n",
        "def agent_writer(mcp_message, client, generation_model):\n",
        "    \"\"\"\n",
        "    Combines research with a blueprint to generate the final output.\n",
        "    UPGRADE: Now accepts the generation_model configuration.\n",
        "    \"\"\"\n",
        "    logging.info(\"[Writer] Activated. Applying blueprint to source material...\")\n",
        "    try:\n",
        "        blueprint_json_string = mcp_message['content'].get('blueprint')\n",
        "        facts = mcp_message['content'].get('facts')\n",
        "        previous_content = mcp_message['content'].get('previous_content')\n",
        "\n",
        "        if not blueprint_json_string:\n",
        "            raise ValueError(\"Writer requires 'blueprint' in the input content.\")\n",
        "\n",
        "        if facts:\n",
        "            source_material = facts\n",
        "            source_label = \"RESEARCH FINDINGS\"\n",
        "        elif previous_content:\n",
        "            source_material = previous_content\n",
        "            source_label = \"PREVIOUS CONTENT (For Rewriting)\"\n",
        "        else:\n",
        "            raise ValueError(\"Writer requires either 'facts' or 'previous_content'.\")\n",
        "\n",
        "        system_prompt = f\"\"\"You are an expert content generation AI.\n",
        "        Your task is to generate content based on the provided SOURCE MATERIAL.\n",
        "        Crucially, you MUST structure, style, and constrain your output according to the rules defined in the SEMANTIC BLUEPRINT provided below.\n",
        "\n",
        "        --- SEMANTIC BLUEPRINT (JSON) ---\n",
        "        {blueprint_json_string}\n",
        "        --- END SEMANTIC BLUEPRINT ---\n",
        "\n",
        "        Adhere strictly to the blueprint's instructions, style guides, and goals. The blueprint defines HOW you write; the source material defines WHAT you write about.\n",
        "        \"\"\"\n",
        "\n",
        "        user_prompt = f\"\"\"\n",
        "        --- SOURCE MATERIAL ({source_label}) ---\n",
        "        {source_material}\n",
        "        --- END SOURCE MATERIAL ---\n",
        "\n",
        "        Generate the content now, following the blueprint precisely.\n",
        "        \"\"\"\n",
        "\n",
        "        # UPGRADE: Pass all dependencies to the robust LLM call.\n",
        "        final_output = call_llm_robust(\n",
        "            system_prompt,\n",
        "            user_prompt,\n",
        "            client=client,\n",
        "            generation_model=generation_model\n",
        "        )\n",
        "\n",
        "        return create_mcp_message(\"Writer\", final_output)\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"[Writer] An error occurred: {e}\")\n",
        "        raise e\n",
        "\n",
        "logging.info(\"âœ… Specialist Agents defined and fully upgraded.\")"
      ],
      "metadata": {
        "id": "pFStlpjzCwNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5.The Agent Registry (The Toolkit)"
      ],
      "metadata": {
        "id": "qXTeBsSdLJBA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#5.The Agent Registry (Final Hardened Version)\n",
        "# -------------------------------------------------------------------------\n",
        "# We now make one final, crucial upgrade to the AgentRegistry.\n",
        "# We must ensure all dependencies, including namespaces, are passed through.\n",
        "# -------------------------------------------------------------------------\n",
        "\n",
        "class AgentRegistry:\n",
        "    def __init__(self):\n",
        "        self.registry = {\n",
        "            \"Librarian\": agent_context_librarian,\n",
        "            \"Researcher\": agent_researcher,\n",
        "            \"Writer\": agent_writer,\n",
        "        }\n",
        "\n",
        "    # *** Updated signature to include namespace_context and namespace_knowledge ***\n",
        "    def get_handler(self, agent_name, client, index, generation_model, embedding_model, namespace_context, namespace_knowledge):\n",
        "        handler_func = self.registry.get(agent_name)\n",
        "        if not handler_func:\n",
        "            logging.error(f\"Agent '{agent_name}' not found in registry.\")\n",
        "            raise ValueError(f\"Agent '{agent_name}' not found in registry.\")\n",
        "\n",
        "        if agent_name == \"Librarian\":\n",
        "            # *** Inject the context namespace into the Librarian handler ***\n",
        "            return lambda mcp_message: handler_func(mcp_message, client=client, index=index, embedding_model=embedding_model, namespace_context=namespace_context)\n",
        "        elif agent_name == \"Researcher\":\n",
        "            # *** Inject the knowledge namespace into the Researcher handler ***\n",
        "            return lambda mcp_message: handler_func(mcp_message, client=client, index=index, generation_model=generation_model, embedding_model=embedding_model, namespace_knowledge=namespace_knowledge)\n",
        "        elif agent_name == \"Writer\":\n",
        "            return lambda mcp_message: handler_func(mcp_message, client=client, generation_model=generation_model)\n",
        "        else:\n",
        "            return handler_func\n",
        "\n",
        "\n",
        "    def get_capabilities_description(self):\n",
        "        \"\"\"\n",
        "        Returns a structured description of the agents for the Planner LLM.\n",
        "        UPGRADE: Now includes an explicit instruction to use exact key names.\n",
        "        \"\"\"\n",
        "        return \"\"\"\n",
        "        Available Agents and their required inputs.\n",
        "        CRITICAL: You MUST use the exact input key names provided for each agent.\n",
        "\n",
        "        1. AGENT: Librarian\n",
        "           ROLE: Retrieves Semantic Blueprints (style/structure instructions).\n",
        "           INPUTS:\n",
        "             - \"intent_query\": (String) A descriptive phrase of the desired style or format.\n",
        "           OUTPUT: The blueprint structure (JSON string).\n",
        "\n",
        "        2. AGENT: Researcher\n",
        "           ROLE: Retrieves and synthesizes factual information on a topic.\n",
        "           INPUTS:\n",
        "             - \"topic_query\": (String) The subject matter to research.\n",
        "           OUTPUT: Synthesized facts (String).\n",
        "\n",
        "        3. AGENT: Writer\n",
        "           ROLE: Generates or rewrites content by applying a Blueprint to source material.\n",
        "           INPUTS:\n",
        "             - \"blueprint\": (String/Reference) The style instructions (usually from Librarian).\n",
        "             - \"facts\": (String/Reference) Factual information (usually from Researcher).\n",
        "             - \"previous_content\": (String/Reference) Existing text for rewriting.\n",
        "           OUTPUT: The final generated text (String).\n",
        "        \"\"\"\n",
        "\n",
        "# Initialize the global toolkit.\n",
        "AGENT_TOOLKIT = AgentRegistry()\n",
        "logging.info(\"âœ… Agent Registry initialized and fully upgraded.\")"
      ],
      "metadata": {
        "id": "SpHqskOtDHlZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6.The Context Engine"
      ],
      "metadata": {
        "id": "lfrKnJ4fPJm8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#6.The Context Engine (Fully Upgraded with Full Dependency Injection)\n",
        "# -------------------------------------------------------------------------\n",
        "# We now complete the upgrade of the engine's core components.\n",
        "# This includes the Planner's JSON output structure and passing\n",
        "# all configuration variables (model names, namespaces) down the execution chain.\n",
        "# -------------------------------------------------------------------------\n",
        "\n",
        "# === 6.1. The Tracer (Upgraded with Logging) ===\n",
        "class ExecutionTrace:\n",
        "    \"\"\"Logs the entire execution flow for debugging and analysis.\"\"\"\n",
        "    def __init__(self, goal):\n",
        "        self.goal = goal\n",
        "        self.plan = None\n",
        "        self.steps = []\n",
        "        self.status = \"Initialized\"\n",
        "        self.final_output = None\n",
        "        self.start_time = time.time()\n",
        "        logging.info(f\"ExecutionTrace initialized for goal: '{self.goal}'\")\n",
        "\n",
        "    def log_plan(self, plan):\n",
        "        self.plan = plan\n",
        "        logging.info(\"Plan has been logged to the trace.\")\n",
        "\n",
        "    def log_step(self, step_num, agent, planned_input, mcp_output, resolved_input):\n",
        "        \"\"\"Logs the details of a single execution step.\"\"\"\n",
        "        self.steps.append({\n",
        "            \"step\": step_num,\n",
        "            \"agent\": agent,\n",
        "            \"planned_input\": planned_input,\n",
        "            \"resolved_context\": resolved_input,\n",
        "            \"output\": mcp_output['content']\n",
        "        })\n",
        "        logging.info(f\"Step {step_num} ({agent}) logged to the trace.\")\n",
        "\n",
        "    def finalize(self, status, final_output=None):\n",
        "        self.status = status\n",
        "        self.final_output = final_output\n",
        "        self.duration = time.time() - self.start_time\n",
        "        logging.info(f\"Trace finalized with status '{status}'. Duration: {self.duration:.2f}s\")\n",
        "\n",
        "# === 6.2. The Planner (Hardened with Structured JSON Output) ===\n",
        "# *** Planner Logic for JSON Mode ***\n",
        "def planner(goal, capabilities, client, generation_model):\n",
        "    \"\"\"\n",
        "    Analyzes the goal and generates a structured Execution Plan using the LLM.\n",
        "    UPGRADE: Explicitly defines the JSON structure for robustness in json_mode.\n",
        "    \"\"\"\n",
        "    logging.info(\"Planner activated. Analyzing goal and generating execution plan...\")\n",
        "\n",
        "    # Updated System Prompt to ensure compatibility with json_mode=True\n",
        "    # We explicitly request a JSON object containing the key \"plan\".\n",
        "    system_prompt = f\"\"\"\n",
        "    You are the strategic core of the Context Engine. Analyze the user's high-level goal and create a structured Execution Plan using the available agents.\n",
        "\n",
        "    --- AVAILABLE CAPABILITIES ---\n",
        "    {capabilities}\n",
        "    --- END CAPABILITIES ---\n",
        "\n",
        "    INSTRUCTIONS:\n",
        "    1. The output MUST be a single JSON object.\n",
        "    2. This JSON object must contain a key named \"plan\".\n",
        "    3. The value of the \"plan\" key MUST be a list of objects, where each object is a \"step\".\n",
        "    4. Be strategic. Break down complex goals into distinct steps.\n",
        "    5. You MUST use Context Chaining. If a step requires input from a previous step, reference it using the syntax $$STEP_X_OUTPUT$$.\n",
        "\n",
        "    EXAMPLE OUTPUT FORMAT:\n",
        "    {{\n",
        "      \"plan\": [\n",
        "        {{\"step\": 1, \"agent\": \"AgentName1\", \"input\": {{\"param1\": \"value1\"}}}},\n",
        "        {{\"step\": 2, \"agent\": \"AgentName2\", \"input\": {{\"param2\": \"$$STEP_1_OUTPUT$$\"}}}}\n",
        "      ]\n",
        "    }}\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        plan_json_string = call_llm_robust(\n",
        "            system_prompt,\n",
        "            goal,\n",
        "            client=client,\n",
        "            generation_model=generation_model,\n",
        "            json_mode=True\n",
        "        )\n",
        "\n",
        "        # Simplified and robust parsing logic (no regex needed when json_mode=True)\n",
        "        try:\n",
        "            plan_data = json.loads(plan_json_string)\n",
        "        except json.JSONDecodeError as e:\n",
        "            logging.error(f\"Planner failed to parse JSON despite json_mode. Raw String: {plan_json_string}\")\n",
        "            raise ValueError(f\"Invalid JSON returned by Planner: {e}\")\n",
        "\n",
        "        # Handle the primary expected case: {\"plan\": [...]}\n",
        "        if isinstance(plan_data, dict) and \"plan\" in plan_data and isinstance(plan_data[\"plan\"], list):\n",
        "            plan = plan_data[\"plan\"]\n",
        "\n",
        "        # Handle edge cases\n",
        "        elif isinstance(plan_data, list):\n",
        "            logging.warning(\"Planner returned a raw list instead of the requested JSON object.\")\n",
        "            plan = plan_data\n",
        "        elif isinstance(plan_data, dict) and \"step\" in plan_data:\n",
        "            logging.warning(\"Planner received a single JSON step object; wrapping it in a list.\")\n",
        "            plan = [plan_data]\n",
        "        else:\n",
        "            # Addresses the original \"The extracted JSON is not a list structure\" error.\n",
        "            logging.error(f\"Planner returned an unexpected JSON structure. Response: {plan_json_string}\")\n",
        "            raise ValueError(\"The extracted JSON does not conform to the expected structure (must be an object containing a 'plan' list).\")\n",
        "\n",
        "        if not plan:\n",
        "             raise ValueError(\"The generated plan is empty.\")\n",
        "\n",
        "        logging.info(\"Planner generated plan successfully.\")\n",
        "        return plan\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Planner failed to generate a valid plan. Error: {e}\")\n",
        "        raise e\n",
        "\n",
        "# === 6.3. The Executor (Fully Upgraded) ===\n",
        "def resolve_dependencies(input_params, state):\n",
        "    \"\"\"Helper function to replace $$REF$$ placeholders with data from the execution state.\"\"\"\n",
        "    resolved_input = copy.deepcopy(input_params)\n",
        "\n",
        "    def resolve(value):\n",
        "        if isinstance(value, str) and value.startswith(\"$$\") and value.endswith(\"$$\"):\n",
        "            ref_key = value[2:-2]\n",
        "            if ref_key in state:\n",
        "                logging.info(f\"Executor resolved dependency '{ref_key}'.\")\n",
        "                return state[ref_key]\n",
        "            else:\n",
        "                raise ValueError(f\"Dependency Error: Reference {ref_key} not found in execution state.\")\n",
        "        elif isinstance(value, dict):\n",
        "            return {k: resolve(v) for k, v in value.items()}\n",
        "        elif isinstance(value, list):\n",
        "            return [resolve(v) for v in value]\n",
        "        return value\n",
        "\n",
        "    return resolve(resolved_input)\n",
        "\n",
        "# *** Updated signature to include namespace_context and namespace_knowledge ***\n",
        "def context_engine(goal, client, pc, index_name, generation_model, embedding_model, namespace_context, namespace_knowledge):\n",
        "    \"\"\"\n",
        "    The main entry point for the Context Engine. Manages Planning and Execution.\n",
        "    \"\"\"\n",
        "    logging.info(f\"--- [Context Engine] Starting New Task --- Goal: {goal}\")\n",
        "    trace = ExecutionTrace(goal)\n",
        "    registry = AGENT_TOOLKIT\n",
        "\n",
        "    # Added robustness: Handle Pinecone index connection safely\n",
        "    try:\n",
        "        index = pc.Index(index_name)\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Failed to connect to Pinecone index '{index_name}': {e}\")\n",
        "        trace.finalize(\"Failed during Initialization (Pinecone Connection)\")\n",
        "        return None, trace\n",
        "\n",
        "\n",
        "    # --- Phase 1: Plan ---\n",
        "    try:\n",
        "        capabilities = registry.get_capabilities_description()\n",
        "        plan = planner(goal, capabilities, client=client, generation_model=generation_model)\n",
        "        trace.log_plan(plan)\n",
        "    except Exception as e:\n",
        "        # The error logging is already handled within the planner, but we finalize the trace here.\n",
        "        trace.finalize(\"Failed during Planning\")\n",
        "        return None, trace\n",
        "\n",
        "    # --- Phase 2: Execute ---\n",
        "    state = {}\n",
        "    for step in plan:\n",
        "        step_num = step.get(\"step\")\n",
        "        agent_name = step.get(\"agent\")\n",
        "        planned_input = step.get(\"input\")\n",
        "\n",
        "        # Added robustness: Validate step structure\n",
        "        if not step_num or not agent_name or planned_input is None:\n",
        "            error_message = f\"Invalid step structure in plan: {step}\"\n",
        "            logging.error(f\"--- Executor: FATAL ERROR --- {error_message}\")\n",
        "            trace.finalize(\"Failed during Execution (Invalid Plan Structure)\")\n",
        "            return None, trace\n",
        "\n",
        "        logging.info(f\"--- Executor: Starting Step {step_num}: {agent_name} ---\")\n",
        "\n",
        "        try:\n",
        "            # *** Pass the namespaces when retrieving the handler ***\n",
        "            handler = registry.get_handler(\n",
        "                agent_name,\n",
        "                client=client,\n",
        "                index=index,\n",
        "                generation_model=generation_model,\n",
        "                embedding_model=embedding_model,\n",
        "                namespace_context=namespace_context,\n",
        "                namespace_knowledge=namespace_knowledge\n",
        "            )\n",
        "\n",
        "            resolved_input = resolve_dependencies(planned_input, state)\n",
        "            mcp_resolved_input = create_mcp_message(\"Engine\", resolved_input)\n",
        "\n",
        "            mcp_output = handler(mcp_resolved_input)\n",
        "\n",
        "            output_data = mcp_output[\"content\"]\n",
        "            state[f\"STEP_{step_num}_OUTPUT\"] = output_data\n",
        "            trace.log_step(step_num, agent_name, planned_input, mcp_output, resolved_input)\n",
        "            logging.info(f\"--- Executor: Step {step_num} completed. ---\")\n",
        "\n",
        "        except Exception as e:\n",
        "            error_message = f\"Execution failed at step {step_num} ({agent_name}): {e}\"\n",
        "            logging.error(f\"--- Executor: FATAL ERROR --- {error_message}\")\n",
        "            trace.finalize(f\"Failed at Step {step_num}\")\n",
        "            return None, trace\n",
        "\n",
        "    # --- Finalization ---\n",
        "    final_output = state.get(f\"STEP_{len(plan)}_OUTPUT\")\n",
        "    trace.finalize(\"Success\", final_output)\n",
        "    logging.info(\"--- [Context Engine] Task Complete ---\")\n",
        "    return final_output, trace"
      ],
      "metadata": {
        "id": "82TB6NsSPBsK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 7.Execution (with the Fully Upgraded and Corrected Engine)\n",
        "# -------------------------------------------------------------------------\n",
        "# This is the final step where we run our fully upgraded and hardened engine.\n",
        "# We now define all configuration variables here and pass them into the\n",
        "# context_engine call, completing our transition to a fully\n",
        "# dependency-aware and self-contained system.\n",
        "# -------------------------------------------------------------------------\n",
        "\n",
        "# Ensure required display imports are present (for Colab/Jupyter)\n",
        "try:\n",
        "    from IPython.display import display, Markdown\n",
        "except ImportError:\n",
        "    logging.warning(\"IPython display not available. Output will be printed as plain text.\")\n",
        "    def display(content):\n",
        "        print(content)\n",
        "    def Markdown(content):\n",
        "        return content\n",
        "\n",
        "\n",
        "logging.info(\"******** Example 1: Executing the Hardened Engine **********\\n\")\n",
        "\n",
        "goal_1 = \"Write a short, suspenseful scene for a children's story about the Apollo 11 moon landing, highlighting the danger.\"\n",
        "\n",
        "# --- Define all configuration variables before the call ---\n",
        "INDEX_NAME = 'genai-mas-mcp-ch3'\n",
        "GENERATION_MODEL = \"gpt-5\"\n",
        "EMBEDDING_MODEL = \"text-embedding-3-small\"\n",
        "\n",
        "# *** Define the Namespaces ***\n",
        "# IMPORTANT: These must match the actual namespaces used in your Pinecone index\n",
        "# as defined in the RAG_Pipeline notebook.\n",
        "# *** UPDATE: Aligning with the RAG_Pipeline definitions ***\n",
        "NAMESPACE_CONTEXT = 'ContextLibrary'\n",
        "NAMESPACE_KNOWLEDGE = 'KnowledgeStore'\n",
        "\n",
        "\n",
        "# --- UPGRADE: Run the Context Engine with full dependency injection ---\n",
        "# We now pass ALL dependenciesâ€”clients, configurations, and namespacesâ€”into the engine.\n",
        "result_1, trace_1 = context_engine(\n",
        "    goal_1,\n",
        "    client=client,\n",
        "    pc=pc,\n",
        "    index_name=INDEX_NAME,\n",
        "    generation_model=GENERATION_MODEL,\n",
        "    embedding_model=EMBEDDING_MODEL,\n",
        "    # *** FIX: Pass the namespaces into the engine ***\n",
        "    namespace_context=NAMESPACE_CONTEXT,\n",
        "    namespace_knowledge=NAMESPACE_KNOWLEDGE\n",
        ")\n",
        "\n",
        "if result_1:\n",
        "    logging.info(\"\\n******** FINAL OUTPUT 1 **********\")\n",
        "    display(Markdown(result_1))\n",
        "\n",
        "    # Optional: Display the detailed trace for debugging\n",
        "    # print(f\"\\nTrace Status: {trace_1.status}\")\n",
        "    # import pprint\n",
        "    # pp = pprint.PrettyPrinter(indent=2)\n",
        "    # pp.pprint(trace_1.steps)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        },
        "id": "WB7udFkcDQBS",
        "outputId": "34565606-a869-4a58-9c9e-256fbe87dc04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "July 20, 1969. The Moon waits. Black sky. Sharp light. Long shadows.\n\nScreens glow on Earth. Millions watch. Breath held.\n\nApollo 11 glides in. Our mission. NASAâ€™s Apollo program. The Space Race has led us here.\n\nEagle hums. Our Lunar Module. Small. Tight. Beeps peck at my ears. Static hisses. I am the commander. Neil. Buzz sits beside me. Calm voice. Quick hands.\n\nAbove us, Michael circles alone. The Command Module is his world. A bright seed against the dark.\n\nThe ground ahead looks rough. Rocks. Craters. Shadows sliding like slow ink. The automatic path is wrong. Too risky.\n\nManual control. My hands on the stick. No autopilot. Just me, the Moon, and the tiny push of engines.\n\nThe cabin trembles. Numbers blink. Fuel falls. Low. Lower. The word pulls tight in my chest.\n\nSilence presses. Only clicks. Only breath. Only the soft roar of the descent engine.\n\nDust rises below. Gray mist, stirred by fire. It hides the surface. Hides what waits. The unknown watches back.\n\nI tilt. I search. I steer past dark holes and bright stones. Buzz calls out. Calm. Clear. We count the seconds. We count the drops of fuel.\n\nEarth listens. A planet leaning toward a whisper.\n\nShadows reach for us. The gauge bites down. Almost empty.\n\nI ease Eagle forward. A touch. Another. We settle.\n\nStillness.\n\nWe have landed. The first humans on the Moon.\n\nIn the quiet, the fear slips away. The mystery stays. The world exhales. The Moon does not."
          },
          "metadata": {}
        }
      ]
    }
  ]
}