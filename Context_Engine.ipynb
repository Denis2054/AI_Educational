{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN+HD4ZjLDZXIWLXQ+LYUgB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Denis2054/AI_Educational/blob/master/Context_Engine.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Context Engine\n",
        "\n",
        "Copyright 2025, Denis Rothman"
      ],
      "metadata": {
        "id": "hfzZQTDjpfu6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.Installation and Setup"
      ],
      "metadata": {
        "id": "-1bEq01K2Nmz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1.Installation and Setup\n",
        "# -------------------------------------------------------------------------\n",
        "# We install specific versions for stability and reproducibility.\n",
        "# We include tiktoken for token-based chunking and tenacity for robust API calls."
      ],
      "metadata": {
        "id": "_MlRuXOwA7Ai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tqdm==4.67.1 --upgrade\n",
        "!pip install openai==1.104.2\n",
        "!pip install pinecone==7.0.0 tqdm==4.67.1 tenacity==8.3.0"
      ],
      "metadata": {
        "id": "NlXCn7y6CQ3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports and API Key Setup\n",
        "# We will use the OpenAI library to interact with the LLM and Google Colab's\n",
        "# secret manager to securely access your API key.\n",
        "\n",
        "import os\n",
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "# Load the API key from Colab secrets, set the env var, then init the client\n",
        "try:\n",
        "    api_key = userdata.get(\"API_KEY\")\n",
        "    if not api_key:\n",
        "        raise userdata.SecretNotFoundError(\"API_KEY not found.\")\n",
        "\n",
        "    # Set environment variable for downstream tools/libraries\n",
        "    os.environ[\"OPENAI_API_KEY\"] = api_key\n",
        "\n",
        "    # Create client (will read from OPENAI_API_KEY)\n",
        "    client = OpenAI()\n",
        "    print(\"OpenAI API key loaded and environment variable set successfully.\")\n",
        "\n",
        "except userdata.SecretNotFoundError:\n",
        "    print('Secret \"API_KEY\" not found.')\n",
        "    print('Please add your OpenAI API key to the Colab Secrets Manager.')\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while loading the API key: {e}\")\n",
        "\n",
        "# Configuration\n",
        "EMBEDDING_MODEL = \"text-embedding-3-small\"\n",
        "EMBEDDING_DIM = 1536 # Dimension for text-embedding-3-small\n",
        "GENERATION_MODEL = \"gpt-5\""
      ],
      "metadata": {
        "id": "R9fssMtAwGlg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47e373f3-11e9-4223-b170-503194871576"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenAI API key loaded and environment variable set successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports for this notebook\n",
        "import json\n",
        "import time\n",
        "from tqdm.auto import tqdm\n",
        "import tiktoken\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "from tenacity import retry, stop_after_attempt, wait_random_exponential\n",
        "# general imports required in the notebooks of this book\n",
        "import re\n",
        "import textwrap\n",
        "from IPython.display import display, Markdown\n",
        "import copy"
      ],
      "metadata": {
        "id": "ptErFjUn54u0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    # Standard way to access secrets securely in Google Colab\n",
        "    from google.colab import userdata\n",
        "    PINECONE_API_KEY = userdata.get('PINECONE_API_KEY')\n",
        "    if not PINECONE_API_KEY:\n",
        "        raise ValueError(\"API Keys not found in Colab secrets.\")\n",
        "    print(\"API Keys loaded successfully.\")\n",
        "except ImportError:\n",
        "    # Fallback for non-Colab environments (e.g., local Jupyter)\n",
        "    PINECONE_API_KEY = os.environ.get('PINECONE_API_KEY')\n",
        "    if not PINECONE_API_KEY:\n",
        "        print(\"Warning: API Keys not found. Ensure environment variables are set.\")"
      ],
      "metadata": {
        "id": "d6V_5MOsBeRl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56f1e2ac-6b26-445e-cd9c-504aca476862"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "API Keys loaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.Initialize Clients"
      ],
      "metadata": {
        "id": "dxctIvv62hOm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.Initialize Clients\n",
        "# --- Initialize Clients (assuming this is already done) ---\n",
        "\n",
        "# --- Initialize Pinecone Client ---\n",
        "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
        "\n",
        "# --- Define Index and Namespaces (assuming this is already done) ---\n",
        "INDEX_NAME = 'genai-mas-mcp-ch3'\n",
        "NAMESPACE_KNOWLEDGE = \"KnowledgeStore\"\n",
        "NAMESPACE_CONTEXT = \"ContextLibrary\"\n",
        "spec = ServerlessSpec(cloud='aws', region='us-east-1')\n",
        "\n",
        "# Check if index exists\n",
        "if INDEX_NAME not in pc.list_indexes().names():\n",
        "    print(f\"Index '{INDEX_NAME}' not found. Creating new serverless index...\")\n",
        "    pc.create_index(\n",
        "        name=INDEX_NAME,\n",
        "        dimension=EMBEDDING_DIM, # Make sure EMBEDDING_DIM is defined\n",
        "        metric='cosine',\n",
        "        spec=spec\n",
        "    )\n",
        "    # Wait for index to be ready\n",
        "    while not pc.describe_index(INDEX_NAME).status['ready']:\n",
        "        print(\"Waiting for index to be ready...\")\n",
        "        time.sleep(1)\n",
        "    print(\"Index created successfully. It is new and empty.\")\n",
        "else:\n",
        "    # This block runs ONLY if the index already existed.\n",
        "    print(f\"Index '{INDEX_NAME}' already exists.\")\n",
        "    print(\"Clearing namespaces for a fresh start...\")\n",
        "\n",
        "    # Connect to the index to perform operations\n",
        "    index = pc.Index(INDEX_NAME)"
      ],
      "metadata": {
        "id": "yqAbeOskEjP4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fbe6cfd-c03b-4c8b-8d34-465ab2e93092"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index 'genai-mas-mcp-ch3' already exists.\n",
            "Clearing namespaces for a fresh start...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.Helper Functions (LLM, Embeddings, and MCP)"
      ],
      "metadata": {
        "id": "YpnNWNgcB_jY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#3. Helper Functions (LLM, Embeddings, MCP, Pinecone)\n",
        "# -------------------------------------------------------------------------\n",
        "# Utility functions to standardize interactions.\n",
        "# -------------------------------------------------------------------------\n",
        "\n",
        "# === LLM Interaction ===\n",
        "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\n",
        "def call_llm_robust(system_prompt, user_prompt,json_mode=False):\n",
        "    \"\"\"A centralized function to handle all LLM interactions with retries.\"\"\"\n",
        "    try:\n",
        "        response_format = {\"type\": \"json_object\"} if json_mode else {\"type\": \"text\"}\n",
        "        response = client.chat.completions.create(\n",
        "            model=GENERATION_MODEL,\n",
        "            response_format=response_format,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": user_prompt}\n",
        "            ],\n",
        "        )\n",
        "        return response.choices[0].message.content.strip()\n",
        "    except Exception as e:\n",
        "        print(f\"Error calling LLM: {e}\")\n",
        "        # Raise the exception so the caller can handle it or the engine can stop\n",
        "        raise e\n",
        "\n",
        "# === Embeddings ===\n",
        "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\n",
        "def get_embedding(text):\n",
        "    \"\"\"Generates embeddings for a single text query with retries.\"\"\"\n",
        "    text = text.replace(\"\\n\", \" \")\n",
        "    response = client.embeddings.create(input=[text], model=EMBEDDING_MODEL)\n",
        "    return response.data[0].embedding\n",
        "\n",
        "# === Model Context Protocol (MCP) ===\n",
        "def create_mcp_message(sender, content, metadata=None):\n",
        "    \"\"\"Creates a standardized MCP message.\"\"\"\n",
        "    return {\n",
        "        \"protocol_version\": \"2.0 (Context Engine)\",\n",
        "        \"sender\": sender,\n",
        "        \"content\": content, # The actual payload/context\n",
        "        \"metadata\": metadata or {}\n",
        "    }\n",
        "\n",
        "# === Pinecone Interaction ===\n",
        "def query_pinecone(query_text, namespace, top_k=1):\n",
        "    \"\"\"Embeds the query text and searches the specified Pinecone namespace.\"\"\"\n",
        "    try:\n",
        "        query_embedding = get_embedding(query_text)\n",
        "        response = index.query(\n",
        "            vector=query_embedding,\n",
        "            namespace=namespace,\n",
        "            top_k=top_k,\n",
        "            include_metadata=True\n",
        "        )\n",
        "        return response['matches']\n",
        "    except Exception as e:\n",
        "        print(f\"Error querying Pinecone (Namespace: {namespace}): {e}\")\n",
        "        raise e\n",
        "\n",
        "print(\"Helper functions defined.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FwEeJUGhCp0-",
        "outputId": "71f790a2-3c2c-4460-b425-227f0a088fbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Helper functions defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 4.The Specialist Agents (The Handlers)\n",
        "# -------------------------------------------------------------------------\n",
        "# We define the specialist agents. These are largely reused from Chapter 3,\n",
        "# but enhanced to handle more flexible inputs required for dynamic planning.\n",
        "# Agents return the raw data (string) as the MCP 'content' for simplicity.\n",
        "# -------------------------------------------------------------------------\n",
        "\n",
        "# === 4.1. Context Librarian Agent (Procedural RAG) ===\n",
        "def agent_context_librarian(mcp_message):\n",
        "    \"\"\"\n",
        "    Retrieves the appropriate Semantic Blueprint from the Context Library.\n",
        "    \"\"\"\n",
        "    print(\"\\n[Librarian] Activated. Analyzing intent...\")\n",
        "    # Extract the specific input required by this agent\n",
        "    requested_intent = mcp_message['content'].get('intent_query')\n",
        "\n",
        "    if not requested_intent:\n",
        "        raise ValueError(\"Librarian requires 'intent_query' in the input content.\")\n",
        "\n",
        "    # Query Pinecone Context Namespace\n",
        "    results = query_pinecone(requested_intent, NAMESPACE_CONTEXT, top_k=1)\n",
        "\n",
        "    if results:\n",
        "        match = results[0]\n",
        "        print(f\"[Librarian] Found blueprint '{match['id']}' (Score: {match['score']:.2f})\")\n",
        "        # Retrieve the blueprint JSON string stored in metadata\n",
        "        blueprint_json = match['metadata']['blueprint_json']\n",
        "        # The output content IS the blueprint itself (as a string)\n",
        "        content = blueprint_json\n",
        "    else:\n",
        "        print(\"[Librarian] No specific blueprint found. Returning default.\")\n",
        "        # Fallback default\n",
        "        content = json.dumps({\"instruction\": \"Generate the content neutrally.\"})\n",
        "\n",
        "    return create_mcp_message(\"Librarian\", content)\n",
        "\n",
        "# === 4.2. Researcher Agent (Factual RAG) ===\n",
        "def agent_researcher(mcp_message):\n",
        "    \"\"\"\n",
        "    Retrieves and synthesizes factual information from the Knowledge Base.\n",
        "    \"\"\"\n",
        "    print(\"\\n[Researcher] Activated. Investigating topic...\")\n",
        "    # Extract the specific input required by this agent\n",
        "    topic = mcp_message['content'].get('topic_query')\n",
        "\n",
        "    if not topic:\n",
        "        raise ValueError(\"Researcher requires 'topic_query' in the input content.\")\n",
        "\n",
        "    # Query Pinecone Knowledge Namespace\n",
        "    results = query_pinecone(topic, NAMESPACE_KNOWLEDGE, top_k=3)\n",
        "\n",
        "    if not results:\n",
        "        print(\"[Researcher] No relevant information found.\")\n",
        "        # Return a string indicating no data found\n",
        "        return create_mcp_message(\"Researcher\", \"No data found on the topic.\")\n",
        "\n",
        "    # Synthesize the findings (Retrieve-and-Synthesize)\n",
        "    print(f\"[Researcher] Found {len(results)} relevant chunks. Synthesizing...\")\n",
        "    source_texts = [match['metadata']['text'] for match in results]\n",
        "\n",
        "    system_prompt = \"\"\"You are an expert research synthesis AI.\n",
        "    Synthesize the provided source texts into a concise, bullet-pointed summary relevant to the user's topic. Focus strictly on the facts provided in the sources. Do not add outside information.\"\"\"\n",
        "\n",
        "    user_prompt = f\"Topic: {topic}\\n\\nSources:\\n\" + \"\\n\\n---\\n\\n\".join(source_texts)\n",
        "\n",
        "    # Use a low temperature for factual synthesis\n",
        "    findings = call_llm_robust(system_prompt, user_prompt)\n",
        "\n",
        "    # The output content IS the findings (as a string)\n",
        "    return create_mcp_message(\"Researcher\", findings)\n",
        "\n",
        "# === 4.3. Writer Agent (Generation) ===\n",
        "def agent_writer(mcp_message):\n",
        "    \"\"\"\n",
        "    Combines the factual research with the semantic blueprint to generate the final output.\n",
        "    Crucially enhanced to handle either raw facts OR previous content for rewriting tasks.\n",
        "    \"\"\"\n",
        "    print(\"\\n[Writer] Activated. Applying blueprint to source material...\")\n",
        "\n",
        "    # Extract inputs.\n",
        "    blueprint_json_string = mcp_message['content'].get('blueprint')\n",
        "    # Check for 'facts' first, then 'previous_content'\n",
        "    facts = mcp_message['content'].get('facts')\n",
        "    previous_content = mcp_message['content'].get('previous_content')\n",
        "\n",
        "    if not blueprint_json_string:\n",
        "         raise ValueError(\"Writer requires 'blueprint' in the input content.\")\n",
        "\n",
        "    # Determine the source material and label for the prompt\n",
        "    if facts:\n",
        "        source_material = facts\n",
        "        source_label = \"RESEARCH FINDINGS\"\n",
        "    elif previous_content:\n",
        "        source_material = previous_content\n",
        "        source_label = \"PREVIOUS CONTENT (For Rewriting)\"\n",
        "    else:\n",
        "        raise ValueError(\"Writer requires either 'facts' or 'previous_content'.\")\n",
        "\n",
        "\n",
        "    # The Writer's System Prompt incorporates the dynamically retrieved blueprint\n",
        "    system_prompt = f\"\"\"You are an expert content generation AI.\n",
        "    Your task is to generate content based on the provided SOURCE MATERIAL.\n",
        "    Crucially, you MUST structure, style, and constrain your output according to the rules defined in the SEMANTIC BLUEPRINT provided below.\n",
        "\n",
        "    --- SEMANTIC BLUEPRINT (JSON) ---\n",
        "    {blueprint_json_string}\n",
        "    --- END SEMANTIC BLUEPRINT ---\n",
        "\n",
        "    Adhere strictly to the blueprint's instructions, style guides, and goals. The blueprint defines HOW you write; the source material defines WHAT you write about.\n",
        "    \"\"\"\n",
        "\n",
        "    user_prompt = f\"\"\"\n",
        "    --- SOURCE MATERIAL ({source_label}) ---\n",
        "    {source_material}\n",
        "    --- END SOURCE MATERIAL ---\n",
        "\n",
        "    Generate the content now, following the blueprint precisely.\n",
        "    \"\"\"\n",
        "\n",
        "    # Generate the final content (slightly higher temperature for potential creativity)\n",
        "    final_output = call_llm_robust(system_prompt, user_prompt)\n",
        "\n",
        "    # The output content IS the generated text (as a string)\n",
        "    return create_mcp_message(\"Writer\", final_output)\n",
        "\n",
        "print(\"Specialist Agents defined.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFStlpjzCwNR",
        "outputId": "3758998b-6515-4b19-900c-be6130f06e6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Specialist Agents defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 5.The Agent Registry (The Toolkit)\n",
        "# -------------------------------------------------------------------------\n",
        "# We formalize the \"Handler Registry\" into an AgentRegistry.\n",
        "# This catalogs agents and describes their capabilities to the Planner.\n",
        "# -------------------------------------------------------------------------\n",
        "\n",
        "class AgentRegistry:\n",
        "    def __init__(self):\n",
        "        # Mapping of agent names to their corresponding functions\n",
        "        self.registry = {\n",
        "            \"Librarian\": agent_context_librarian,\n",
        "            \"Researcher\": agent_researcher,\n",
        "            \"Writer\": agent_writer,\n",
        "        }\n",
        "\n",
        "    def get_handler(self, agent_name):\n",
        "        \"\"\"Retrieves the function associated with an agent name.\"\"\"\n",
        "        handler = self.registry.get(agent_name)\n",
        "        if not handler:\n",
        "            raise ValueError(f\"Agent '{agent_name}' not found in registry.\")\n",
        "        return handler\n",
        "\n",
        "    def get_capabilities_description(self):\n",
        "        \"\"\"\n",
        "        Returns a structured description of the agents for the Planner LLM.\n",
        "        This is crucial for the Planner to understand how to use the agents.\n",
        "        \"\"\"\n",
        "        return \"\"\"\n",
        "        Available Agents and their required inputs:\n",
        "\n",
        "        1. AGENT: Librarian\n",
        "           ROLE: Retrieves Semantic Blueprints (style/structure instructions).\n",
        "           INPUTS:\n",
        "             - \"intent_query\": (String) A descriptive phrase of the desired style or format.\n",
        "           OUTPUT: The blueprint structure (JSON string).\n",
        "\n",
        "        2. AGENT: Researcher\n",
        "           ROLE: Retrieves and synthesizes factual information on a topic.\n",
        "           INPUTS:\n",
        "             - \"topic_query\": (String) The subject matter to research.\n",
        "           OUTPUT: Synthesized facts (String).\n",
        "\n",
        "        3. AGENT: Writer\n",
        "           ROLE: Generates or rewrites content by applying a Blueprint to source material.\n",
        "           INPUTS:\n",
        "             - \"blueprint\": (String/Reference) The style instructions (usually from Librarian).\n",
        "             - \"facts\": (String/Reference) Factual information (usually from Researcher). Use this for new content generation.\n",
        "             - \"previous_content\": (String/Reference) Existing text (usually from a prior Writer step). Use this for rewriting/adapting content.\n",
        "           OUTPUT: The final generated text (String).\n",
        "        \"\"\"\n",
        "\n",
        "# Initialize the global toolkit\n",
        "AGENT_TOOLKIT = AgentRegistry()\n",
        "print(\"Agent Registry initialized.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8HzVg_IMC_yw",
        "outputId": "5a960a9d-83e0-47e2-9d26-d62a201311d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Agent Registry initialized.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 6.The Context Engine (Planner, Executor, Tracer)\n",
        "# -------------------------------------------------------------------------\n",
        "# This is the core innovation of Chapter 4. It replaces the linear\n",
        "# Orchestrator with a dynamic, LLM-driven planning and execution system.\n",
        "# -------------------------------------------------------------------------\n",
        "\n",
        "# === 6.1. The Tracer (Debugging Implementation) ===\n",
        "class ExecutionTrace:\n",
        "    \"\"\"Logs the entire execution flow for debugging and analysis.\"\"\"\n",
        "    def __init__(self, goal):\n",
        "        self.goal = goal\n",
        "        self.plan = None\n",
        "        self.steps = []\n",
        "        self.status = \"Initialized\"\n",
        "        self.final_output = None\n",
        "        self.start_time = time.time()\n",
        "\n",
        "    def log_plan(self, plan):\n",
        "        self.plan = plan\n",
        "\n",
        "    def log_step(self, step_num, agent, planned_input, mcp_output, resolved_input):\n",
        "        \"\"\"Logs the details of a single execution step.\"\"\"\n",
        "        self.steps.append({\n",
        "            \"step\": step_num,\n",
        "            \"agent\": agent,\n",
        "             # The raw input definitions from the plan (including $$REFS$$)\n",
        "            \"planned_input\": planned_input,\n",
        "            # Crucial for debugging: What exact context did the agent receive?\n",
        "            \"resolved_context\": resolved_input,\n",
        "            \"output\": mcp_output['content']\n",
        "        })\n",
        "\n",
        "    def finalize(self, status, final_output=None):\n",
        "        self.status = status\n",
        "        self.final_output = final_output\n",
        "        self.duration = time.time() - self.start_time\n",
        "\n",
        "    def display_trace(self):\n",
        "        \"\"\"Displays the trace in a readable format.\"\"\"\n",
        "        display(Markdown(f\"### Execution Trace\\n**Goal:** {self.goal}\\n**Status:** {self.status} (Duration: {self.duration:.2f}s)\"))\n",
        "        if self.plan:\n",
        "            # Display the raw plan JSON\n",
        "            display(Markdown(f\"#### Plan:\\n```json\\n{json.dumps(self.plan, indent=2)}\\n```\"))\n",
        "\n",
        "        display(Markdown(\"#### Execution Steps:\"))\n",
        "        for step in self.steps:\n",
        "            print(f\"--- Step {step['step']}: {step['agent']} ---\")\n",
        "            print(\"  [Planned Input]:\", step['planned_input'])\n",
        "            # print(\"  [Resolved Context]:\", textwrap.shorten(str(step['resolved_context']), width=150))\n",
        "            print(\"  [Output Snippet]:\", textwrap.shorten(str(step['output']), width=150))\n",
        "            print(\"-\" * 20)\n",
        "\n",
        "\n",
        "# === 6.2. The Planner (Strategic Analysis) ===\n",
        "def planner(goal, capabilities):\n",
        "    \"\"\"\n",
        "    Analyzes the goal and generates a structured Execution Plan using the LLM.\n",
        "    \"\"\"\n",
        "    print(\"[Engine: Planner] Analyzing goal and generating execution plan...\")\n",
        "    system_prompt = f\"\"\"\n",
        "    You are the strategic core of the Context Engine. Analyze the user's high-level goal and create a structured Execution Plan using the available agents.\n",
        "\n",
        "    --- AVAILABLE CAPABILITIES ---\n",
        "    {capabilities}\n",
        "    --- END CAPABILITIES ---\n",
        "\n",
        "    INSTRUCTIONS:\n",
        "    1. The plan MUST be a JSON list of objects, where each object is a \"step\".\n",
        "    2. You MUST use Context Chaining. If a step requires input from a previous step, reference it using the syntax $$STEP_X_OUTPUT$$.\n",
        "    3. Be strategic. Break down complex goals (like sequential rewriting) into distinct steps. Use the correct input keys ('facts' vs 'previous_content') for the Writer agent.\n",
        "\n",
        "    EXAMPLE GOAL: \"Write a suspenseful story about Apollo 11.\"\n",
        "    EXAMPLE PLAN (JSON LIST):\n",
        "    [\n",
        "        {{\"step\": 1, \"agent\": \"Librarian\", \"input\": {{\"intent_query\": \"suspenseful narrative blueprint\"}}}},\n",
        "        {{\"step\": 2, \"agent\": \"Researcher\", \"input\": {{\"topic_query\": \"Apollo 11 landing details\"}}}},\n",
        "        {{\"step\": 3, \"agent\": \"Writer\", \"input\": {{\"blueprint\": \"$$STEP_1_OUTPUT$$\", \"facts\": \"$$STEP_2_OUTPUT$$\"}}}}\n",
        "    ]\n",
        "\n",
        "    EXAMPLE GOAL: \"Write a technical report on Juno, then rewrite it casually.\"\n",
        "    EXAMPLE PLAN (JSON LIST):\n",
        "    [\n",
        "        {{\"step\": 1, \"agent\": \"Librarian\", \"input\": {{\"intent_query\": \"technical report structure\"}}}},\n",
        "        {{\"step\": 2, \"agent\": \"Researcher\", \"input\": {{\"topic_query\": \"Juno mission technology\"}}}},\n",
        "        {{\"step\": 3, \"agent\": \"Writer\", \"input\": {{\"blueprint\": \"$$STEP_1_OUTPUT$$\", \"facts\": \"$$STEP_2_OUTPUT$$\"}}}},\n",
        "        {{\"step\": 4, \"agent\": \"Librarian\", \"input\": {{\"intent_query\": \"casual summary style\"}}}},\n",
        "        {{\"step\": 5, \"agent\": \"Writer\", \"input\": {{\"blueprint\": \"$$STEP_4_OUTPUT$$\", \"previous_content\": \"$$STEP_3_OUTPUT$$\"}}}}\n",
        "    ]\n",
        "\n",
        "    Respond ONLY with the JSON list.\n",
        "    \"\"\"\n",
        "    # Call LLM in JSON mode for reliability\n",
        "    plan_json = \"\"\n",
        "    try:\n",
        "        plan_json = call_llm_robust(system_prompt, goal, json_mode=True)\n",
        "        plan = json.loads(plan_json)\n",
        "\n",
        "        # Validate the output structure\n",
        "        if not isinstance(plan, list):\n",
        "             # Handle cases where the LLM wraps the list in a dictionary (e.g., {\"plan\": [...]})\n",
        "             if isinstance(plan, dict) and \"plan\" in plan and isinstance(plan[\"plan\"], list):\n",
        "                 plan = plan[\"plan\"]\n",
        "             else:\n",
        "                raise ValueError(\"Planner did not return a valid JSON list structure.\")\n",
        "\n",
        "        print(\"[Engine: Planner] Plan generated successfully.\")\n",
        "        return plan\n",
        "    except Exception as e:\n",
        "        print(f\"[Engine: Planner] Failed to generate a valid plan. Error: {e}. Raw LLM Output: {plan_json}\")\n",
        "        raise e\n",
        "\n",
        "\n",
        "# === 6.3. The Executor (Context Assembly and Execution) ===\n",
        "\n",
        "def resolve_dependencies(input_params, state):\n",
        "    \"\"\"\n",
        "    Helper function to replace $$REF$$ placeholders with actual data from the execution state.\n",
        "    This implements Context Chaining.\n",
        "    \"\"\"\n",
        "    # Use copy.deepcopy to ensure the original plan structure is not modified\n",
        "    resolved_input = copy.deepcopy(input_params)\n",
        "\n",
        "    # Recursive function to handle potential nested structures\n",
        "    def resolve(value):\n",
        "        if isinstance(value, str) and value.startswith(\"$$\") and value.endswith(\"$$\"):\n",
        "            ref_key = value[2:-2]\n",
        "            if ref_key in state:\n",
        "                # Retrieve the actual data (string) from the previous step's output\n",
        "                print(f\"[Engine: Executor] Resolved dependency {ref_key}.\")\n",
        "                return state[ref_key]\n",
        "            else:\n",
        "                raise ValueError(f\"Dependency Error: Reference {ref_key} not found in execution state.\")\n",
        "        elif isinstance(value, dict):\n",
        "            return {k: resolve(v) for k, v in value.items()}\n",
        "        elif isinstance(value, list):\n",
        "            return [resolve(v) for v in value]\n",
        "        return value\n",
        "\n",
        "    return resolve(resolved_input)\n",
        "\n",
        "\n",
        "def context_engine(goal):\n",
        "    \"\"\"\n",
        "    The main entry point for the Context Engine. Manages Planning and Execution.\n",
        "    \"\"\"\n",
        "    print(f\"\\n=== [Context Engine] Starting New Task ===\\nGoal: {goal}\\n\")\n",
        "    trace = ExecutionTrace(goal)\n",
        "    registry = AGENT_TOOLKIT\n",
        "\n",
        "    # Phase 1: Plan\n",
        "    try:\n",
        "        capabilities = registry.get_capabilities_description()\n",
        "        plan = planner(goal, capabilities)\n",
        "        trace.log_plan(plan)\n",
        "    except Exception as e:\n",
        "        trace.finalize(\"Failed during Planning\")\n",
        "        # Return the trace even in failure for debugging\n",
        "        return None, trace\n",
        "\n",
        "    # Phase 2: Execute\n",
        "    # State stores the raw outputs (strings) of each step: { \"STEP_X_OUTPUT\": data_string }\n",
        "    state = {}\n",
        "\n",
        "    for step in plan:\n",
        "        step_num = step.get(\"step\")\n",
        "        agent_name = step.get(\"agent\")\n",
        "        planned_input = step.get(\"input\")\n",
        "\n",
        "        print(f\"\\n[Engine: Executor] Starting Step {step_num}: {agent_name}\")\n",
        "\n",
        "        try:\n",
        "            handler = registry.get_handler(agent_name)\n",
        "\n",
        "            # Context Assembly: Resolve dependencies\n",
        "            resolved_input = resolve_dependencies(planned_input, state)\n",
        "\n",
        "            # Execute Agent via MCP\n",
        "            # Create an MCP message with the RESOLVED input for the agent\n",
        "            mcp_resolved_input = create_mcp_message(\"Engine\", resolved_input)\n",
        "            mcp_output = handler(mcp_resolved_input)\n",
        "\n",
        "            # Update State and Log Trace\n",
        "            output_data = mcp_output[\"content\"]\n",
        "\n",
        "            # Store the output data (the context itself)\n",
        "            state[f\"STEP_{step_num}_OUTPUT\"] = output_data\n",
        "            trace.log_step(step_num, agent_name, planned_input, mcp_output, resolved_input)\n",
        "            print(f\"[Engine: Executor] Step {step_num} completed.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            error_message = f\"Execution failed at step {step_num} ({agent_name}): {e}\"\n",
        "            print(f\"[Engine: Executor] ERROR: {error_message}\")\n",
        "            trace.finalize(f\"Failed at Step {step_num}\")\n",
        "            # Return the trace for debugging the failure\n",
        "            return None, trace\n",
        "\n",
        "    # Finalization\n",
        "    final_output = state.get(f\"STEP_{len(plan)}_OUTPUT\")\n",
        "    trace.finalize(\"Success\", final_output)\n",
        "    print(\"\\n=== [Context Engine] Task Complete ===\")\n",
        "\n",
        "    # Return the output of the final step AND the trace\n",
        "    return final_output, trace"
      ],
      "metadata": {
        "id": "SpHqskOtDHlZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 7.Execution (Standard Goal)\n",
        "# -------------------------------------------------------------------------\n",
        "# Demonstrate the engine with a standard goal similar to Chapter 3,\n",
        "# showing how the Planner dynamically constructs the workflow.\n",
        "# -------------------------------------------------------------------------\n",
        "\n",
        "print(\"******** Example 1: STANDARD WORKFLOW (Suspenseful Narrative) **********\\n\")\n",
        "goal_1 = \"Write a short, suspenseful scene for a children's story about the Apollo 11 moon landing, highlighting the danger.\"\n",
        "\n",
        "# Run the Context Engine\n",
        "# Ensure the Pinecone index is populated (from Ch3 notebook) for this to work.\n",
        "result_1, trace_1 = context_engine(goal_1)\n",
        "\n",
        "if result_1:\n",
        "    print(\"\\n******** FINAL OUTPUT 1 **********\\n\")\n",
        "    display(Markdown(result_1))\n",
        "    print(\"\\n\\n\" + \"=\"*50 + \"\\n\\n\")\n",
        "    # Optional: Display the trace to see the engine's process\n",
        "    # trace_1.display_trace()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WB7udFkcDQBS",
        "outputId": "db7d8a72-544c-4b0d-e1c2-383f7e16c76c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "******** Example 1: STANDARD WORKFLOW (Suspenseful Narrative) **********\n",
            "\n",
            "\n",
            "=== [Context Engine] Starting New Task ===\n",
            "Goal: Write a short, suspenseful scene for a children's story about the Apollo 11 moon landing, highlighting the danger.\n",
            "\n",
            "[Engine: Planner] Analyzing goal and generating execution plan...\n",
            "[Engine: Planner] Plan generated successfully.\n",
            "\n",
            "[Engine: Executor] Starting Step 1: Librarian\n",
            "\n",
            "[Librarian] Activated. Analyzing intent...\n",
            "[Librarian] Found blueprint 'blueprint_suspense_narrative' (Score: 0.66)\n",
            "[Engine: Executor] Step 1 completed.\n",
            "\n",
            "[Engine: Executor] Starting Step 2: Researcher\n",
            "\n",
            "[Researcher] Activated. Investigating topic...\n",
            "[Researcher] Found 2 relevant chunks. Synthesizing...\n",
            "[Engine: Executor] Step 2 completed.\n",
            "\n",
            "[Engine: Executor] Starting Step 3: Writer\n",
            "[Engine: Executor] Resolved dependency STEP_1_OUTPUT.\n",
            "[Engine: Executor] Resolved dependency STEP_2_OUTPUT.\n",
            "\n",
            "[Writer] Activated. Applying blueprint to source material...\n",
            "[Engine: Executor] Step 3 completed.\n",
            "\n",
            "=== [Context Engine] Task Complete ===\n",
            "\n",
            "******** FINAL OUTPUT 1 **********\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "July 20, 1969. Apollo 11. NASA’s Apollo program. The Space Race hums in the background like a distant storm.\n\nThree of us. Neil Armstrong. Buzz Aldrin. Michael Collins.\n\nThe Moon fills the window. Black sky. Hard light. Long shadows.\n\nNumbers creep. The guidance drifts. The margin thins. Fuel is low.\n\nI take manual control of the Lunar Module Eagle. The stick is firm. My glove is steady. The engine’s tone is a thin, living wire.\n\nThe cabin is small. A soft hiss in my ears. Breathing. Fans. A tremor underfoot. Shadows crawl over the ground and hide the truth of it.\n\nWe descend. Slow. Slower. The surface slides by, gray and strange. The target shifts. I nudge us. Left. Forward. Hold.\n\nThe world listens. Broadcast live. A billion quiet rooms. A billion held breaths.\n\nThe hum rises, falls. The Eagle steadies. The scene tightens to a pin.\n\nThen—stillness.\n\nSilence folds in. My heartbeat steps out of time and back again. The Moon waits, wide and wordless.\n\nI look down at that pale ground. Distance, then inches. I move carefully. Deliberate. Every motion a decision.\n\nThe hatch. The edge. The black beyond the light. I feel the weight change. Not heavy. Not free. Just different.\n\nA boot hovers. I watch the shadow first. It kisses the surface before I do.\n\nThen contact. A first step. On the Moon.\n\nThe world is there with me, far away and everywhere at once. The air in my suit whispers. The surface holds. The moment stretches thin, like a note, and does not break."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "==================================================\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}